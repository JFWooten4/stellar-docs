WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:59.999 align:start position:0%
okay I think we have like a couple of things on the agenda so I think the first one was around please to kind of give a quick update of on where we are and then like we have a follow-up discussions on the archiving work that Garen has been working on maybe I can start with yeah face given that this is like a quick thing so yeah so like we have we started to kind of go back to the that fee cap that I was looking at the draft it was first put together end of June last year so it has been sitting accumulating dust for a long time and yeah so we're actually like looking into this again we started a by doing a couple things so first one

00:01:00.000 --> 00:01:59.999 align:start position:0%
was around [Music] refreshing the cap to reflect as much as possible where we left it where it left things off in the on the mailing list there's plenty of still of open questions and yeah we're going to go over that I think the other thing was that we started a thread on this code on the channels so yeah at this point it's more like yeah we need to go through and iterate on this before we can give more updates on on what's going to be next on the feed front and basically like the yeah what we're realizing is that there are a lot of things that we didn't incorporate yet like there are you know this was written before we had for example the pre-flight in the picture things like that so

00:02:00.000 --> 00:02:59.999 align:start position:0%
yeah there will be a for sure a little more a few more changes that need to have to be done but that's kind of what we are with this not everyone to kind of spend more time on this just like you know I'm inviting people to to kind of follow the conversations and yeah talk about and maybe help on you know helping us making decisions on this one do we maybe want to keep a free high level overview of what is there and you know what are the open questions we are trying to solve a recent page let's spend more time on the archival proposal yeah I think that yeah like so I guess yeah good good point like the type of things we are trying to

00:03:00.000 --> 00:03:59.999 align:start position:0%
big issues I would say that we need to to kind of converge on on the feed front is on what type of experience we want to expose to contract developers when it comes to the different markets that that exist in the system so we have the different resource types we have a Ledger space we have a compute so like when when transactions execute we have a network bandwidth and NBS I think in terms of C markets that's Canada we have other things that are related more to external systems so like when people for example produce metadata that then gets consumed by systems like horizon or sort of an OPC or yeah like still are

00:04:00.000 --> 00:04:59.999 align:start position:0%
expert in red like uses this kind of of a data stream so like making it that people can just spam those other systems is is part of the you know in school for for the the fee schedule and then what did I forget oh yeah we have also archives what has basically answered mandatory right like published to those history archives so making sure so that people don't use that as they are alternative to S3 or you know like other places that you know when people can store data if they want want to the difference here is that those history archives are romantic analysis forever after so there are some constraints there and yeah so like the type of problems around those this the the model for fees is how do we make it that we

00:05:00.000 --> 00:05:59.999 align:start position:0%
can balance usability so right now you know people used to do the classic system they have a very simple way to think about things basically you have a base fee for one operation if your transaction contains more than one you just multiply and that's kind of your your base in a way to kind of think think about the you know in terms of fee and if you want to get ahead of other transactions on the network for whatever reason it just increase your your feeling that's kind of it right and in soroban like the because of the competition between those different resource types that are open-ended right in terms of consumption and competition so yeah we're going to need something a little bit better than that I mean like they're like in

00:06:00.000 --> 00:06:59.999 align:start position:0%
places like in you know Italian they have you know a version of what you can do do with these so this is a single fee for for everything there are like proposals to make it maybe okay it comes to to different resource types but that's nothing yet in Italian it is implemented in other on cases like polkadot I think [Music] but yeah that's why we are kind of trying to get something usable okay I think that's kind of what I wanted to talk about on the feed from just you know heads up it's coming let's see and yeah we have the next guarant that I think wanted to give us a little more updates on last time there was like we started to talk about the

00:07:00.000 --> 00:07:59.999 align:start position:0%
archive mechanism that allows to to save space on the Ledger so that we can keep the network as cheap as as possible and I think there were a few interesting follow-up conversations that also happen after that in this Garden of this goal so Aaron and give us maybe a little a few updates on what's going on there yeah so I guess first I want to talk a little or just have some time for questions about the interface that we talked about last week and so kind of just like a high level summary of what we went over last time is essentially all sort of on data has this rent fee and this rent balance and so every Ledger or periodically you have to pay rent for keeping an entry live on The Ledger and then whenever an entry runs out of its rent

00:08:00.000 --> 00:08:59.999 align:start position:0%
balance it could still be in from The Ledger and then sent to the archive and so with that interface we've kind of exposed three different classes of storage these kind of three different types of storage replace today what is currently the storage layer which is like end dot storage in your smart contract code and so with these three types of storage Ares we have a unique storage which is there's only ever one version of the entry that exists the entry either exists on the bucket list or there's a single version of that entry on the archive but never both and this is useful for types of data that have security concerns such as nonsense or certain types of authorization where there could be security risks and issues if you have multiple versions of that entry that could be restored kind of the the use case here is if you could think about Implement a nonce where you didn't have this unique storage guarantee you could find yourself where you have a

00:09:00.000 --> 00:09:59.999 align:start position:0%
version one of the nonce in the archive of like say value five and then version two of the nonce and the archive with a different value and then you can imagine how a malicious user could restore those entries in such a way that your knots values out of date and not the correct value that should be so that's unique data it's more expensive because you have to prove that something doesn't exist in the archive whenever you create something new and so there's a little bit of work that needs to be done so it's the most expensive data type but it's reserved for like those security and high-risk sort of entries and then after unique storage we have what's called recreatable storage which is a similar in that recreable storage entries whenever you run out of rent balance also get sent to the archive the only difference is that recreatable storage might have different versions in the archiver multiple different versions exists at the same time the reason for this is that whenever you create a recreatable storage entry you don't check the archive to see if something already exists there and so say you have

00:10:00.000 --> 00:10:59.999 align:start position:0%
something like a balance that got archived and then you go to create a new version of that after your old key got archived in recreatable storage you don't check the archive and so you just create a new entry with the exact same key and so you have this key collision and so that's it's a little cheaper than unique storage because you don't have to check the archive and actually show that this entry is unique that could be multiple versions of it so it's cheaper but it's not appropriate for security types such as like nonsense or auth where you don't want multiple versions and so that's unique storage and recreatable storage both of which can be archived and then the final type of storage is called temporary storage and this are for short-lived entries and so whenever a temporary storage entry runs out of rent it just gets deleted it doesn't get sent to the archive and so temporary storage is an appropriate for sensitive data that you want to keep around like user balances but can be useful for data types that either don't need to live very often like a short-term authorization to let an

00:11:00.000 --> 00:11:59.999 align:start position:0%
address spend your funds for instance or for data types that can be easily recreated if they get deleted such as like a payment path or a payment Channel or something like that and so I think now even I think first I just want to open up the floor for questions and to talk about this kind of like a interface and in particular talk about this like three-tiered approach and having three different classes of storage because I know that there's a little controversial and it's definitely a little bit more complex on the current interface just wondering if there are any questions that I do have a question if I can go for it so this is in my sense this is actually a very good design I like it excuse me I'm wondering in terms of staging this work I I it's quite complicated and involved in some some quantity of it is going to depend on

00:12:00.000 --> 00:12:59.999 align:start position:0%
some pretty big components being built out in terms of the archivers which is fine and and I think we can I think we can do some staging I'm I'm in the sense of you know deploying versions of zoroban that have the interface but but you know some of it is just defined to do nothing at this point or that sort of thing I'm a little bit concerned about the unique storage one because unique right off the bat has to have these these exclusion proofs in order to do any rights is that correct yes I think what we could do is we can still do a staging process right and so I think what this would probably look like in practice is whenever we launch sort button we don't have the archive built out and so kind of the current plan for you know v0 on launch is to have the interface set so to expose the unique recreable and temporary storage entry types to the user and then to charge rent and so the thing that won't be

00:13:00.000 --> 00:13:59.999 align:start position:0%
there though is that whenever your rent balance goes to zero you won't get delayed and you won't get sent to the archive because the archive won't be built yet right and so I think for Unique data in particular what we probably just want to do is we can still I think we still should launch unique data and recreable storage just so that contracts can have the correct Paradigm written at launch but what we can do I think at the implementation level is just the four just special cases right so like before we have these proofs we can just say essentially like creating unique entries does not require proof of exclusion until we actually provide an interface for those proof of exclusions that's that's exactly the part I was asking about is is do you think we would just sort of make it an optional field at first and then when we rev the protocol the optional field has to have a value in it and we'll Define what that value is in the future well we could do that we're actually now that I'm thinking about it what we could do is just zero initialize through cache and say the root archive hash is null and then proof of exclusion become

00:14:00.000 --> 00:14:59.999 align:start position:0%
trivial right because if your hash is null then you're guaranteed that it's empty and so I actually think we can provide proofs of exclusion on day Zero actually if we just Define the the null hash and so the proof will always be null but that's about proof if your root hash is null okay the other thing I'm thinking is that you want this you want to use uniques for nonsense and it seems like we we update nonsense quite a lot and so I'm a little bit concerned about sort of an expensive operation that involves constructing a proof of exclusion has to adhere to every every nonce update certainly I I wonder about the nonsense that are maintained by the auth system maybe Dima could speak to that I think the third yeah I can I'm sorry if you don't mind current so I think guaranteed is using non's example is well just an example of why this problem

00:15:00.000 --> 00:15:59.999 align:start position:0%
matters for the built-in nonsense at least we decided to move forward with a temporary knowns approach or nonsense and signatures basically have some time boundaries and certain the temporary Ledger entries so they don't even run again and I would say this should be a preferred approach right like I reiterated this several times in the discussions here on Discord that with the existence of temporary storage it's a really good idea to try benefit from it and try to design for it right like in this case of nonsense right we can have this we need to bump it multiple times and it's not super convenient and stuff and I I think the main use case for The Unique storage is really some admin data like you really don't want your admin entry to be taken over by someone just

00:16:00.000 --> 00:16:59.999 align:start position:0%
because it has expired right you have a token contract that you have insured once a year ago and you have never touched it but you don't want to wake after a year the rent has expired you don't really want someone else to just re-initialize it because the entry has expired so I feel like this is the main use case and this is a really pretty cold entries if you think about it right so yeah I think thinking about this a bit I I I sort of retract my concern because I think you're right that if if the system has a well-defined notion of temporary storage with with time limits on it then you just time bound anything that is you you propagate those time bounds to the things that use that storage such that they they would become invalid at the same moment so I I can see that being quite quite a viable approach I like that that's good thank you I also once make one additional clarification I think in your original question you said oh do you have to like provide a proof of exclusion every time you update it and that's not true you only have to provide the proof at creation time

00:17:00.000 --> 00:17:59.999 align:start position:0%
and then of course you have to provide proofs if you run out of rent and then get sent to the archive but once it's actually live on the bucket list then it says just as if you're modifying any other entry the proofs only apply if it's not on the bucket list or if you're creating something for the first time and so if it's like again this is just an example but like if it's a nonsense that's regularly used then it wouldn't matter it would be very cheap and efficient because it would never get to the archive okay and oh I have one other very minor question and this is more of a design like time to hit the thesaurus we already have something in a system called an archive I I just feel like we gotta use a different word for this because it's just gonna it's just gonna follow up a lot of things that are already referred our guys yeah maybe like deep State source and like the yeah I don't know what you're gonna call it but maybe maybe not start with deep state that might be a tough a graveyard yeah cool so I guess any other questions about specifically the recreatable temporary and then unique storage

00:18:00.000 --> 00:18:59.999 align:start position:0%
interface before you move on yes I'd like to ask why like is there any reason why you don't use something like a bloom filter to to check whether the entry already exists in the arch Avenue because given given the notion that we have a bloom filter of all entries in the archive we can probably avoid having multiple versions of archived entries and I think having only one version in the archive and preventing users to recreate some entries that already exist in the archive makes a lot of sense easier yeah so we definitely investigated this

00:19:00.000 --> 00:19:59.999 align:start position:0%
a lot and we tried to find a way if there is a way for validators to store keys or to at least have some knowledge of what's in the archive I think there's a couple issues there so first the goal of the archival state is to bound the amount of storage that values need to store and so if they need to store a key even though that's you know less than storing the entire data entry that's still an unbounded storage so that's issue number one but I think in particular you'd have to have a set of keys that grows unbounded and that's not a great solution because especially for sorbonne data types there's a lot of instances where the key is actually as big or larger than the value because if you can think like the keys are 32 bytes and I'm not you know super in depth at the current sort of implementation so correct me if I'm wrong my understanding is keys are 32 bytes but the value for instance could be something as small as I can that's only like four bytes or something like that and so I think you're not getting as good cost savings as you think if you just store the keys business set not to your Bloom filter

00:20:00.000 --> 00:20:59.999 align:start position:0%
question or if there's a way to store the keys in a more efficient manner the issue with Bloom filters in particular is that they don't it's very difficult to resize the bloom filter and so if you say I have Unbound State and say okay we're going to pick a bloom filter that's one gigabyte large but in 10 years you need a larger Bloom filter because you're getting a lot of collisions and stuff it's impossible to just resize that bloom filter without having all the values because whenever you change the size of the bloom filter you have to rehash everything that you've added to the bloom filter which would not be possible for the validators because they've thrown all those values away and so essentially to resize your Bloom filter have to replay history from the beginning of time in order to resurface all the values that need to be in that blend filter and additionally there's still issues with Bloom filters because they're probabilistic in nature and so you would still have key collisions sometimes or I guess or not Keyhole rather because they

00:21:00.000 --> 00:21:59.999 align:start position:0%
only return they don't return false Nexus I don't think but then there'd be certain keys that just based on the probabilistic nature of the bloom filter the validators would think they're in the archive even though they weren't in the archive and so that would also be an issue where just based on whatever your hash function is there'd be certain keys that would essentially be impossible to create because the balloon filter thinks they already exist when they really don't so that answers your question about the the bloom filter issue in particular the issue in general as well I'm not entirely sure that the resizing issue is such a huge problem because you have an archive where you have all the keys so once in a let's say 10 years it's possible to organize a resizing using the archives as a source of all the

00:22:00.000 --> 00:22:59.999 align:start position:0%
existing keys and to make like a maintenance for all the validators as for the size of the bloom filter for example I just checked and one billion entries with a reasonable false positives like Point 0.1 percent probability of false negatives takes about two and a half gigabytes so it's not the clutch and it will be enough for the first billion entries I think it's a reasonable trade-off for to avoid some other complexities as for the false positives case I think since you have

00:23:00.000 --> 00:23:59.999 align:start position:0%
the vehicle tree or some other structure if it's not set and so on yet then you can probably check whether their hive this is archive contains the given K give given key if it's let's say it can be a resurrected or something like this maybe I'm missing something because I haven't think about this for quite a long time but do you still think that using some Bloom filters other probabilistic structures won't help to prevent collisions of entries in archives like maybe there is some other option because yeah this point looks like one of the most controversial things about

00:24:00.000 --> 00:24:59.999 align:start position:0%
the archives to me yeah this is a an interesting idea actually I think you mentioned that perhaps use the bloom filter as like a almost a caching layer for efficiency and then using the proofs as kind of like a back end in case you get a false positive I'm still not 100 sure if the false positive case can be avoided right because say like from this would be very frustrating from a user standpoint say I have a key or like there's some like deterministic way to like Define cues right so like I give my address and then like the address is input that generates the keys for entries associated with my account right like in a token contract if one of those keys is a false positive then you just won't be able to create any entries based on your your the invoker address right and so I don't know if there's a way if you get a false positive to somehow track the archive and say oh actually that wasn't

00:25:00.000 --> 00:25:59.999 align:start position:0%
a false positive it really is it really doesn't exist I promise and so I'm not sure if there is a way to get around that case because I think like even if you say like a reasonable false positive rate like a 0.1 percent that still means that one out of I think a thousand Keys or maybe a ten thousand keys I might be off by zero or something there but essentially like one of the Thousand Keys you'll think it's in the archive when it really isn't which means that you are not allowed to create one of the thousand one out of a thousand Keys which I feel like could be a really significant issue from the user interface perspective do you think that the time of checking the existence of the key over the Merkel tree or the miracle trios or the structure you plan to use for archive proofs it's like

00:26:00.000 --> 00:26:59.999 align:start position:0%
a really huge time like seconds minutes or even more because if it's relatively small then checking checking the existence only for conflicting entries and you will be checking computer and entries only when someone tries to create already existing entry which actually I shouldn't be as often operation so like what are the time requirements for checking against the try also I think one clarification here is that you can't just track against the archive directly because the archivers don't participate in consensus

00:27:00.000 --> 00:27:59.999 align:start position:0%
and are not validators they are off chain right and so you can't just like search through the tree brute force and trust that the contents are correct so there has to be some way for the archivers to give a proof to validators that the validators can then validate themselves and so I think that's why part of the reason we're using this try model where we can get both proofs of explosion pros of inclusion because I think the difference between this case and the Ubuntu case is that in the Ubuntu case you can trust the the archive in our case we can't trust the archive we have to have some proof via like a hash or something and so I'm still I think it could be interesting this Bloom filter approach if we can do the bloom filter and then if there's a collision maybe say even though there's a collision in the bloom filter I'm going to go and get a proof of exclusion and that would perhaps make it more efficient and mean that in most cases

00:28:00.000 --> 00:28:59.999 align:start position:0%
you can get away with creating entries without a perfect exclusion but I'm still not sure what that would look like I guess maybe what we could do is you create your entry you check the bloom filter and then if you get an error on the bloom filter then you go try to get proof of exclusion and if there's a valid proof of exclusion on chain that can override the bloom filter that might be an interesting optimization but I guess that's second point for explanation I don't want to steal the time it's definitely like I need to make more research on this and probably we should get back on this in that sales matter to to instill the time on this question here thanks for the explanation yeah I think that's a I think it's definitely an interesting idea that's definitely worth pursuing I guess one question I would ask is

00:29:00.000 --> 00:29:59.999 align:start position:0%
under this proposal of the bloom filter and whatnot this would make all data unique data and so I'm wondering if there are use cases where a user might actually want data to be recreatable so thinking of the balanced use case I'm thinking like is there ever a scenario where it's actually an advantage to have multiple different versions as opposed to only having this strict one unique key per archive per bucket list so I'm thinking like for instance in the case of balances where the multiple different versions of the balance can just be summed together is that a Advantage and is there like a do we still want to expose a repradable storage interface so you can be even faster and I guess like not have to do this like Bloom filter check not because of exclusion all that sort of stuff or is like a strict guarantee one key no collisions powerful and that we shouldn't expose this interface at all

00:30:00.000 --> 00:30:59.999 align:start position:0%
to tell the truth from my experience there is probably a known cases when you need several versions of the same major entry in most cases it's even a destructive problem right because creating an account or something else may [Music] be a huge problem I I I'd say that the use case for like optional recreating or maintaining several versions of the same Ledger entry is a rather and I haven't seen any use cases for this and just one question that basically from the contract

00:31:00.000 --> 00:31:59.999 align:start position:0%
to host interface tend to point like there is no version in any Video Edit to kind of work around some issues in the current approach but in general the interfaces that like you put something into storage and it will stay there quote unquote forever or you know you put it into Temp Storage then it will be removed after a certain time period but you know like even from the period is interface standpoint seriously no case for this multiple entry versions and I think the main reason they are thinking of hiding it is just to kind of save some time and cost just to be able to quickly create entries without proof of Proof Set exclusion but it's more like an implementation detail that unfortunately the contract writers would need to worry about in some cases versus like you know something is there things

00:32:00.000 --> 00:32:59.999 align:start position:0%
that can be used as a feature or the contractor correct like if you need any sort of person you can build it yourself using just or as key so what not so I'm pretty sure there is no legitimate case for recreatable storage Beyond it like or requirements to the scalability yeah like maybe I can add that the reason right now the the reason we are looking at this recruitable storage is that we have token balances that are interestingly one of the kind of primary use cases for fast at all right and if we don't have recreatable storage we basically have either Temporaries right entries which for

00:33:00.000 --> 00:33:59.999 align:start position:0%
balance is a no-go or going with those unique entries and for that you need troops to create the balance so the cost of an overhead of just kind of setting up your wallet becomes you know quite quite big for any new token that's kind of the the problem here is that I think the overhead of proofs is probably acceptable the first time you kind of create your own wallet on your on the network but anytime you add a balance for any token it seems that having this overhead is kind of too much but yeah maybe we that's kind of part of this discussion right to see you know are we wrong here yeah so I guess the the trade-off and with the bloom filter approach where

00:34:00.000 --> 00:34:59.999 align:start position:0%
you know and let's just put aside like the resizing and migration issues for now but the bloom filter approach all data is unique but the false positive rate is the percentage of the time that you will need to provide a proof of exclusion for creating new entry and so let's just I guess the trade-off is everything is unique and the interface is easier but one out of a thousand Creations are very slow and require process of create precept explosion whereas if we have unique data and recreatable data then the unique data is guaranteed to always be slow but the recreatable data is guaranteed to always be fast and so I guess the trade-off is do we want all data to be fast most of the time or all data creation to be fast most of the time and sometimes to be really slow for the easier user interface or have a more complex user interface where one type of data is always slow to create and one type of data is guaranteed always fast to create I guess that's the the fundamental trade-off at least in my mind

00:35:00.000 --> 00:35:59.999 align:start position:0%
and that sounds about right like the thing about the blue tilt the other is that if in the context of like a balance right the ID the the key right of that balance is actually deterministic so as it's deterministic it becomes kind of attackable unless we can come up with like a cryptographic you know Broomfield of sorts it's it's very easy to basically you know cause certain keys to be to have complex in the boom filter and then you're kind of back to that you know even though it's one in a thousand you know if you're the one that is always hit by the one it kind of sucks what if we utilize B tree index or some other index or like database actually doing this

00:36:00.000 --> 00:36:59.999 align:start position:0%
and besides that the index itself can reside on the disk and the Fast Cash can be implemented using the bloom filter and the actual track will be carried over the index for example B3 index database handle this I mean the issue with an index is we're getting to that issue where if we have any deterministic index like that we need to store the keys right and then we have that same issue of unbalanced State growth and especially restore upon data where the keys can sometimes be significantly larger to the value so I think any like deterministic data structure we can't get back into that issue of we have unbalanced State versus defeats the purpose of the archive in the first place yeah I I just want to remind way basically we still need to maintain

00:37:00.000 --> 00:37:59.999 align:start position:0%
consensus and we cannot just like randomly update boom shooter for example right it has to be a part of consensus so it would need to come up with some way to Hash it quickly and add it to the CP values and make sure it's archived properly you know significant amount of work and I mean you could say that the keys are stroke in The Ledger forever and then you build some sort of index on them blockchain but then you know yeah it kind of no longer fulfills the requirement of having limited Venture state cross which we wanted to fulfill so it's kind of an issue and yeah that for what source like it was my first city as well like what if we just throw keys in The Ledger but yeah that unfortunately kind of doesn't scale as well yeah so I think the Bloom

00:38:00.000 --> 00:38:59.999 align:start position:0%
filter with a proof of exclusion fallback for false positives it's an interesting idea I think we probably have some technical homework to do there so I think it's all right for everyone to move on to the second topic I'm not hearing any objection Celtic as yes so kind of slipping away from the user interface now talking about how the archiver interface will be set up and so currently there are kind of like two proposals one where we have an archive interface that's a functions similar to capture how Horizon functions now where you go to a specific archiver you have some URL endpoint and then you query that endpoint with the keys you want to be archived and so this is like a a model similar to what we have today with Verizon some of the pros there's pros and cons what are the cons that you have to have like a personal relationship or at least know an archiver to go to and then it's not super clear how we

00:39:00.000 --> 00:39:59.999 align:start position:0%
could incentivize or monetize this sort of interface perhaps you would have to like pay a monthly subscription to the archive or perhaps you'd have some relationship where you like pay your archive or per entry lookup or something like that but it's not super clear how we incentivize people actually brought in archivers in this setup so the second scenario is where we have kind of what I'm referring to as the archive miners kind of stealing the minor terminology from Bitcoin so essentially how this would work is instead of acquiring an archiver directly whenever you need a proof you submit a proof request on chain now this could either be implemented at the protocol level where proof request is an operation or this might also be able to be implemented by like a first party smart contract but that's not really important right now but essentially you would just submit an operation that requests an archival proof and then by submitting this

00:40:00.000 --> 00:40:59.999 align:start position:0%
operation you'd submit meta information that archivers could then adjust and that's how the archivers would know about the requests and as part of this operation you would say the key you want to be proven the type of proof select proof of inclusion proof exclusion and then also a reward and this reward would be variable and you would be the would be at the user's discretion as to what to set this reward to and then this operation this request would go on chain and the metal would be a submit and so an archival within Injustice meta and then pick what requests they want to service and so they could reservice the request that has the highest reward first and then they construct the proof with the information that they store and then they submit the proof another operation and then the proof itself becomes an entry on The Ledger and so once that proof has been submitted and validated and the proof is on Ledger then the proof or the proof request is

00:41:00.000 --> 00:41:59.999 align:start position:0%
deleted and the reward is given to whoever submitted the correct archival proof first and so this is I think a better interface because it has a clearly defined incentive structure and also doesn't require any personal relationships with an archiver and so you don't have to have a URL that you talk to or you don't have to have a relationship with some company that you pay monthly to pay some subscription fee in order to access the archive it also allows archivers to freely enter and exit the network as they please and also by having a variable reward that the user can set you can also have essentially like a built-in supply and demand Dynamics where that price fluctuates over time depending on how many people want to restore archived entries and how many archivers want to service archives and so I guess generally speaking what are your thoughts on the two approaches and kind of the leading approach being this I'm submitting proof request to the

00:42:00.000 --> 00:42:59.999 align:start position:0%
chain and then having archivers read the chain and then submit the proofs how do we feel about that have you ever speak let's see so like the the thing I'm thinking of right in terms of like meaningful viable product I'm thinking the having a way to talk directly to archives is is kind of a foolproof the approach where you use the on-chain state I think I mean it's I think there is like good potential there I think it's it's going to be fairly tricky to get this right the reason being that basically your so you have you're not creating like intrinsic

00:43:00.000 --> 00:43:59.999 align:start position:0%
value right to certain transactions that are being published on the overlay and therefore a [Music] like a board of source right can can look at this overlay traffic and don't run take the data right that is the primage right the proof and from the the archival that actually did the work and benefit from the academy so I think there is like an interesting problem there to solve in terms of like how can you safely disclose the the proof to the network without being from sinus right interesting some entities that can be signing it refining well the issue that was signing is how can like the mental not just

00:44:00.000 --> 00:44:59.999 align:start position:0%
like take your proof and then sign with its own address and then submit as if it was the originator yeah that's fair I'm not sure and I think there might be ways to do it right like it's a maybe what you you it's like a multiple multi-step thing right where you you because you were first to disclose let's say the harsh of the proof before you actually disclosure proof then you're the one you know if it's a contract that's doing the that work then we can basically give the the first you know first one the benefit I mean at the same time like a yeah maybe a bot can I mean it becomes

00:45:00.000 --> 00:45:59.999 align:start position:0%
kind of a a cat and you know a nice game right like where you yeah how to do this safely yeah I think in the front room like because you know yours are like super nice people that maybe on I mean not trying to game the system yeah I thought like the front running I did was just to like if they're the same proofs multiple the same proofs in the same block just or randomly pick one but I didn't think about this proof stealing case in front running by stealing proof so this is definitely an interesting issue to think about but I still like the model where you don't have to have a relationship with the archive for a couple of reasons so first it's likely that especially you know if the archive systems run for a long time archivers will not store the entire archive I think it's a good idea to let the archive pick and choose how much or

00:46:00.000 --> 00:46:59.999 align:start position:0%
how little of his or of the archive history they want to store and so I can see an archiver that only stores the last five years one that stores the last 10 years and then one that's like a more expensive on the stores like the last 50 years for instance I think if you have to like individually query an archiver you have this weird interface where for things that are three years old you can maybe query a cheap archiver and then for things that are older you have to change your URL or something like that to Target like a different archive that has more history state or something like that and so I think there's still some some interface issues with having to talk to the archiver directly but at least on the top of my head I don't have a great solution to the stealing proofs thing I think that's the challenges yeah I was going to say like the I think it's it's a we should definitely be looking into those mechanisms that are like a little more distributed right from a you know Discovery Point of View

00:47:00.000 --> 00:47:59.999 align:start position:0%
I think all it means is that we use we we have the proper semantics on network to allow for for doing this so like I think for example like the thing where we have proofs that are usable independently of using you know the entries yeah I think this is like a a key property that we need to have right because I I know like one of the earlier drafts was requiring people to submit proofs in the same transaction that they were going to use the you know the actual and actually restore the entry and obviously this this would not work in you know would not enable the type of of scenarios yeah I definitely like having the or the proofs themselves be Ledger

00:48:00.000 --> 00:48:59.999 align:start position:0%
entries I just I think we just need to find the the best way to to make sure the system isn't getting it or gained yeah there's a because there's not a clear solution because I mean you could submit say like before you submit the proof If you submit the hash to say hey I was here first and then submit the proof of the next Ledger but then you could open yourself up to Dos attacks for a malicious user could just generate a bunch of dummy proofs and then submit them for every archive request and then archivers would not want to service those requests because something's already spoken for it and issues like that so so I think we need to think about but I definitely like I think we definitely should have the proofs on chain like you mentioned and we should see if there's a way to solve this issue in a way that makes sense if you have a you know a few more minutes left was there like some other topic that you wanted to cover as part of this yes I guess one other

00:49:00.000 --> 00:49:59.999 align:start position:0%
question that's kind of that we don't have a great solution for and this is one that we need to figure out here pretty soon because it's required to launch for v0 is how to bump rent and so right now whenever you create an entry it's created and initializes rent balance to some amount but there doesn't seem to be a great way in order to bump rent and to actually increase that rent balance and so kind of the initial thought was Hey whenever you access something increase the rent and that's way that the things that are accessed most commonly automatically have their rent increased and so if you access something a lot it will most likely be on the buy policy and we'll have to unarchive it now for read write items it's easy because you have to rewrite the entry anyway so you might as well bump the rent however it's not clear how to bump rent on read only items so for instance if you have auth say like an auth record that's almost never changed but is read often

00:50:00.000 --> 00:50:59.999 align:start position:0%
you would want to bump the rent on reads so that you wouldn't have to constantly unarchive it the issue is because the way the bucket list is structured there's no way to bump rent without rewriting the entry because essentially the way the bucket list is structured entries and buckets are immutable and so in order to update an entry it's not like SQL you can just go to the entry and then change a value you have to completely rewrite the entry and so we wouldn't want to bump rent on every read because then we're implicitly at the systems level turning reads into read writes which we don't want to do and so I guess in the read write case we obviously want to bump rent but I was wondering if there are any ideas as to what to do for like read-only data and how to handle rent in that regard well simple way I've been thinking about is well you just said with online but you

00:51:00.000 --> 00:51:59.999 align:start position:0%
know I'm exposed just some contract function but that's free tried access to the entries that you want to bomb and nothing else that will basically you know to call just as any other contract function to bounce around but that maybe can be generalized to you know post functions that allows you action arbitrary entries without accessing them so that you know you don't need to maintain any invariants in terms of like only the contract can modify its own data I guess they can all agree that one pins or end is always positive right so anyone can do that and then you know just kind of cost functions it takes a bunch of majorities catches them at print right supposing pumpkins rent by whatever mechanism they come up this

00:52:00.000 --> 00:52:59.999 align:start position:0%
which again is not super pretty but that makes it possible to do the pump without touching the contract code and increasing inside guys and so on so you know it's a basically a generic way to maintain your contracts I think it's maybe viable yeah I think the host function could be good I think the only issue with that is key Discovery is still an issue and then it might be difficult to determine what keys you need to bump around for in the host function but perhaps one another thing is we could expose a explicit rent bump at like the storage interface and then also make the current rent balance readable in the smart contract so I could imagine something like maybe a common Paradigm for read-only data would be

00:53:00.000 --> 00:53:59.999 align:start position:0%
like if you have an off entry whenever you read it you say you also have a check to see if the rent balance has fallen below some level and if so then you call Rent bump on that value so I guess or I don't know if that would be possible then because or none of that would be possible and so then essentially there's a g path and an expensive path and so if your rent is above the value then pre-flight would put that entry in the read-only data set but if your rent is below a value then PreFlight would put it in the read write set and bump the rent and so I think that might be also another possible solution for for often like read-only data I mean that's kind of possible but doesn't solve it for everything like imagine again your token right and it has the you know an admin and let's say you don't mean this token much so you don't touch the admin entry frequently and then it would still expire right you know the admin

00:54:00.000 --> 00:54:59.999 align:start position:0%
functions frequently enough so you know for the important things you it sure needs some sort of manual tracking and you still need to understand which entries need to be updated they I'm sure we can avoid this food just because you know some interest may be rarely accessible well I think that's okay actually because if you don't access admin very much and whatever you access it you need to restore your admin entry I think that's okay I think the case I'm talking about is if you have some really value that you access a ton that still gets archived all the time yeah but but my point is that you know you can still forget to call it so I mean rely on automatic bumps is not going to always work and another thing is that imagine you know oh maybe it's not a super good example but I I've been just thinking about who who pays for the bump right it's always a source account who pays for the bumps

00:55:00.000 --> 00:55:59.999 align:start position:0%
and it would be a bit weird like if every once in a while like some transaction to your contract suddenly becomes more expensive because you need to bounce around but you may be bump into rent of some entries that has nothing to do with your account specifically like for example let's say you have some found that stores some sorry some contracted Source some State saying it's like never going to change right and it's always free download and then there is no clear owner of the token paywrites they're not owned by any address or anything so what would happen is like every once in a while someone who trades with this liquidity pool will need to pay for their inbound I was a token payer record

00:56:00.000 --> 00:56:59.999 align:start position:0%
which is a little bit awkward right because I I just want to trade why would it charge me more and you know I then draw all the incentives to kind of try to gain this and president not submit transactions until someone else has pumped you know I mean it's definitely viable on paper but it just leads to distribute situations for you know you're in a bumping rent on the some interest you can respond to know about right I guess it also depends on the amount because it's well enough and it probably doesn't matter but if it's high enough then suddenly it becomes pretty annoying for the user who ended up paying it yeah thank you I agree like it we probably need to think about those couple of angles like the people that want to kind of maintain their run balance on those

00:57:00.000 --> 00:57:59.999 align:start position:0%
like read-only type of items that and when I say people here could be a contract that tries to do like like an mm right that wants to kind of ensure that it keeps alive its own thing at the same time there are probably scenarios where you want to kind of do that from the outside in some way right because you don't want every contract I mean yeah in the up cases where this is going to not work like if nobody is using it or and you need to revalue it or I don't know anyways we have time so thank you everybody let's continue those conversations actually we should probably create like explicit threads on the you know on the dev Channel about those topics and then yeah we'll keep going thank you again
