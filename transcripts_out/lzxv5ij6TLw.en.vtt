WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:59.999 align:start position:0%
hello everyone welcome to this week's protocol meeting we have a couple caps to discuss today and I'll start by presenting cap 67 I'll share the link here so this cap is about emitting events in Stellar in the same format that we see in sorine for Stellar assets so that a user can use the stream of events track balances and this cap covers three separate High higher level changes they're listed in the abstract of the cap and I'll first go over the the two smaller changes that make some semantic fixes to the seller asset contract events for compatibility with set 41 which is the token interface Set so the first is that we're going to stop emitting the admin on the mint and clawback events the admin is an implementation detail of the the sa and does not belong in the event the admin doesn't even exist in sub 41 as a concept

00:01:00.000 --> 00:01:59.999 align:start position:0%
the second change is that we're going to update an edge case with regards to transfers to and from an issuer the sa was written in a way that matched classic payment semantics where a transfer from an issuer Min the asset and a transfer to an issuer burns the asset the issue is that in these two scenarios we still emit a transfer event instead of the appropriate Mint or burn event so this cap just proposes fixing that to Mint the the mint the correct Mint or burn event instead of the transfer prevent so I'll pause there real quick if there are any questions about those two changes before I move on to the the bigger change at hand all right then I'll move on to the third and most significant change of this cap which is emitting an event in the same format as the Stellar asset contract for any asset movement and this means that a a stellar operation that results in the movement of an asset can emit

00:02:00.000 --> 00:02:59.999 align:start position:0%
a transfer mint burn or clawback event and we're also adding a fee event to represent the fee paid by by The Source count so the the cap specifies what what the events for each operation will look like but I'll just mention some interesting points so a trade that occurs through either through an offer operation or path payment operation will result in two events one for each side of the trade and the two parties on both events will be the source account of the operation and the owner of the offer and it's and these these events can be transfer events but they can also be instead be a mint or burn if the issuer is involved we're also adding new SD address types to be able to represent the claimable balances and liquidity pools as from or two addresses in these events and this allows us to represent movements to and from these entries

00:03:00.000 --> 00:03:59.999 align:start position:0%
yeah so silence asked can you explain removing the admin from the sa mint and Clack events So currently on the mint and Clack events of the sa not not in set 41 the admin is one of is is one of the topics of the event but this is this isn't necessary and the concept of an admin is not required for a token exactly yeah it's not we're not removing the admin from the sa just it just says the topic we're removing it from the the topic of the event all right so yeah we're also in addition to those two new address types for clal balances and liquidity pools we're also adding another SC address type for MX accounts so that these new events can pass along the MX information that the MX account is used in the operation or transaction it's also another thing to

00:04:00.000 --> 00:04:59.999 align:start position:0%
sh screens so I'm I'm actually not presenting this in the order that it like it shows up in the cap this is just some points I thought were interesting but if you look at cap 67 these points should be listed there at a with more detail then yeah so I wanted to mention that these new events will not be Into The Ledger therefore will not be part of the protocol and we also will emit the emit these events from Genesis if you replay from Genesis and something to keep in mind here is that there was a bug in a very old protocol version where xlm was minted and burned and we will admit events that will allow you to reconcile those Balan changes and finally we will be reworking the format of TX meta with the transaction meta V4 that generalizes the events and separates them by operation

00:05:00.000 --> 00:05:59.999 align:start position:0%
previously the events were for formatted in a way where was very specific to sorban and the fact that sorban only has for a single sbon transaction there's only one operation but the transaction meta V4 generalizes that and the plan is to provide a config flag that enables the emission of this new meta format okay so let me look at some of these questions so will this resolve the issue with a locked issuer sa having an admin Sil can do things the issuer can't lock no so I think what you're sounds I think what you're asking is if you change the admin and you lock the issuer the ad okay ignore that yeah they the admin can still do whatever it wants

00:06:00.000 --> 00:06:59.999 align:start position:0%
to to what else does back filling mean that replaying with these new events enabled will emit meta4 for all ledgers rather than just post 23 yeah that is the goal and we and that should work I we we probably have some stuff to work through there but if we want to admit these events from the beginning then it has to be meta V4 see all right so MX addresses and memo so yeah one one of the issues is so the cap addresses the M partially by forwarding the M addresses into the events but there is another question about memos that I'll link this discussion

00:07:00.000 --> 00:07:59.999 align:start position:0%
topic topic from from Nico it's relevant and at a high level this this amounts to considering a solution where you can group an arbitrary memo with a group of trans events this will allow you to you know solve some issues we've had with in the past with regards to the design of memos but that but yeah I don't know if Lee if you want to discuss this a little bit more that's something this concept is something we still have to discuss yeah good yeah so yeah NCO may want to he he this is sort of I feel

00:08:00.000 --> 00:08:59.999 align:start position:0%
like his idea so he may want to speak to this as well but there you know we have the separate we have another cap cap 64 that we're not actually discussing today that's doing some things with MOS it's touching MOS and where they show up where they will show up in tooling and you know making it possible possible to do a soran transaction with a memo and have that safely included in the signature so that's cap 64 and something that niik was bringing up with this cap is that you know right now we're in this moment where we're making some significant changes to events or we're doing a lot of work around events and doesn't make sense you know we have to sort of we we sort of have to get close to Memos because we've got this issue with moer dresses and you know right now cup 67 67 in a very non-controversial way is just

00:09:00.000 --> 00:09:59.999 align:start position:0%
suggesting that these events now might contain addresses that are MX addresses so we're just going to pass it through so if you do a payment operation that's got a destination of a Max address that address field in the San event could be a MX address it could be an M address instead of a g address or a c address and I think that is like very not noncontroversial it's sort of like straight line what we I guess we would sort of expect the cap to do in the situation but there is this we have this history where you know once upon a time there was memos then there were M addresses right now the ecosystem is pretty there's you know both get used in the ecosystem so both getting used by exchanges and but when you look at the data tells a somewhat confusing story because this is memo field there's this MOX address they can be in the destination you know technically sources can also be

00:10:00.000 --> 00:10:59.999 align:start position:0%
addresses and nik's Nik was sort of suggest I don't know if I don't know if this was actually a proposal but he was suggesting like do we do something more here do we make these events in this moment more opinionated about what the memo for this event is and you know do we do something like if there's a transaction memo this event has a memo is or if there's a destination of a m address do we pull the memo out of the address and we say well actually that's the memo for this event to some degree like the ecosystem has to do this somewhere and you know right now it has to do this Downstream ecosystem tooling has to make a decision about where these events come from and in the future like you know right now memo memos there's like MX addresses are only a classic thing memos only occur at the transaction level in cap 64 but in the future like how do you attach

00:11:00.000 --> 00:11:59.999 align:start position:0%
memos to like other groupings or transfers if there's you know transfers being bundled together yeah sorry I'm not sure I'm actually doing a great job of summarizing this there's like a lot yeah yeah it's definitely something that still needs to be thought through but I guess the question is should we explore yeah addressing that issue in cap 67 it sounds like we should at least at a high level I think we should and I think what nio just posted in the chat is a really succinct way of for the why we should consider it now and that is that you know cap 64 is forcing the memo to be exposed in down TR systems for San events and so it makes sense since this cap is very focused on that consistency between classic and sorban and fixing it across those two dimensions of you know San

00:12:00.000 --> 00:12:59.999 align:start position:0%
versus classic sack versus set 41 it makes sense to discuss now yeah makes sense okay orbit asked this question why did we decide to extend the meta with the vents if these events can be commuted dynamically from the existing meta seems like redundant data in the meta and this is something we actually discussed internally I believe George actually mentioned the same single source of Tru truth rather than risking bugs Downstream renting the wheel and dynamic compute also Lee do you have I say you mut yourself do you want to address this yeah I can speak to this the a large part of this is that you know when we look at the tooling that we have today so we have a horizon which takes a lot of What's in the mattera today and creates this data model for people to consume this new data model for people consume and then St RPC was designed with a different

00:13:00.000 --> 00:13:59.999 align:start position:0%
approach where the Stellar IPC really exposes the raw network data and tries to introduce as little as possible in terms of its own data model on top of that and one of the reasons that Stell IPC was designed that way is that you can go to you know you can take contract events that have come out of star RPC and they look the same way as if you go and take meta out of core or if you use G Galaxy to you know generate a data Lake of meta like the events look the same way in all these different places except if you go to Horizon you know Horizon has its own data model and it has that effects data model which is you know very different so the idea between the ibr like how like why bring these events to Classic and the the the Stellar asset contract and the CL operations is to create this like one

00:14:00.000 --> 00:14:59.999 align:start position:0%
unified view that appears everywhere that looks the same everywhere that that the data occurs okay I hopefully that answers that question but if not let us know I think Lee you mentioned another thing about another question about asset names on the the topic of the events right because the currently go ahead maybe before we move on it does sound like orbit has some things I'm Orit do you want to do you want to speak to yeah hello everyone just wanted to

00:15:00.000 --> 00:15:59.999 align:start position:0%
add my two Sense on this we've been successfully reconstructing everything from transaction meta xdr on the client side basically our inje pipeline in Stell expert is built on top of the library that we built specifically for this purpose and we had a conversation with the Horizon team before like several years ago I think where I proposed to actually remove all this excessive data tables like effects and everything like this because all of it can be reconstructed on the client side of course there are very specific issues like the memo case lay covered before that

00:16:00.000 --> 00:16:59.999 align:start position:0%
but maybe we can just like conduct some research on it and maybe the Parson Library we already have and which have been tested for years and which works with classic and San maybe it's enough because on the client side people can just use it directly in JavaScript to parse the response they receive from DC DC Horizon now doesn't support transaction meta so it's probably case when we cannot use it directly but at least from RPC it has zero problems is

00:17:00.000 --> 00:17:59.999 align:start position:0%
Parson looks like George has something to say but I can't bring him up on stage there oh okay yeah I actually didn't have anything to say I think it's a I think or makes a good point but I do think that a unified event stream is better for like your average dap developer even though we can do all the indexing on the Fly it's probably more catered towards like infrastructure providers like Stellar expert and Horizon so this is still going to have value for people who are just interacting with RPC directly I mean we've been using it to actually display transactions in Sellar Sellar expert be wallet reflector

00:18:00.000 --> 00:18:59.999 align:start position:0%
refractor refractor everywhere I'd say it's a universal approach that can be used everywhere I'm not insisting on it it just seems to me that transaction meta will be even larger after that and it might have some duplicated data because of of this can you hear me well yeah I can hear you yeah okay great so I have a couple thoughts on this first if the meta size increase is concerned I think I don't know if we made it explicit anywhere but I think this should be optional for the most part part at least I don't see a good reason for not making Z

00:19:00.000 --> 00:19:59.999 align:start position:0%
so I don't think and the same goes actually for The Ledger changes ideally you would be able to pick one or another depending on what is your Downstream Downstream processing whatever suits you then regarding the library I wanted to point out that well a not wa JavaScript is not the only language out there but also there is like some Nuance to like there is very Nuance difference between Ledger divs and events because events tell you what exactly has happened during the transaction whereas weder deeps tell you what was the state after the transaction and I think it matters more for soran but I don't know it might have some interesting implications for classic I don't know if they're interesting to anyone but just that the format of the event like if you're processing Samson you

00:20:00.000 --> 00:20:59.999 align:start position:0%
might be interested in processing Samson as a stream of specifically events right and not a stream of some data types that depend on the operations like can see from the library correct me if only them missing something there but this don't seem to me like this stand and the idea here is that you know if I wanted to track just the movement of the balance and I not particularly interested in the exact semantics of each and every store operation what I could do is I could just ingest the event stream and track the balance for a given account which I think may be interesting to some consumers so yeah this are just some thoughts on what why buer doing this at all I think the argument regarding

00:21:00.000 --> 00:21:59.999 align:start position:0%
the event stream is pretty solid because this way the server in this case RPC handles the streaming itself and indexing and cses and Etc so instead of working with transactions and Parson transactions it can like really rely on these events it's a solid argument it point is that I'm not proposing to use JavaScript everywhere maybe we can like Port this to rust or other languages and use it inside RPC inside other applications instead of adding the data we can like do it on the fly in RPC itself right regenerate this events because

00:22:00.000 --> 00:22:59.999 align:start position:0%
basically all described events that what we do with u with the transaction M xdr that's something we already do so maybe it's just one of the options instead of extending the xdr itself again because to me it looks already pretty pretty large my subjective opinion on this but again if you could opt out of imian events would that solve your issue with the size because it's I think besides the size concern I feel like the suggestion of let's put Comm implementation somewhere we just

00:23:00.000 --> 00:23:59.999 align:start position:0%
suggesting well why don't we put this common implementation right into ore I don't think it makes a huge ideological difference and I think it does make something easier to maintain and to ensure that they works correctly for b yeah especially if we are doing protocal changes so yeah and just for the say I totally agree and yeah so I don't know if you have any strong arguments but I feel like you're more or less in the same page here in terms of standardization because it I guess just in of putting it into cor kind of make sense yeah the standardization it sits inside The Narrative of everything being a contract so I think like that's what's beautiful about this is that you know

00:24:00.000 --> 00:24:59.999 align:start position:0%
today we have you know classic operations and then we have everything that happens on sorban and the result of those things looks pretty different but as a result of this work we're moving towards this future where actually everything that just happens on classic looks exactly like a contract that's executing and you know all assets actually do have a contract that's reserved for them they have the Stellar asset asset contract and so the events are just going to look like everything that happens with assets no matter if it's classic or anything like that it just looks like a contract which is also much more similar to other ecosystems other like blockchain ecosystems as well so there's that familiarity for everybody about how this works versus say ethereum and other things yeah and I I'll make sure to clarify in the the cap that nothing's being removed from meta and this will be optional that's a good point I mentioned the config flag for

00:25:00.000 --> 00:25:59.999 align:start position:0%
transaction met V4 but I I'll go into that in a little more detail are there any other questions I didn't quite catch if we have made any final decision on MOS like what we talked about like 10 minutes ago wa wait what is the final stages because I maybe I've lost it yeah so we're for in the context of this cap we are the we're forwarding the M account information right into the the addresses but for memos we still need to explore what Nico mentioned earlier like so there's nothing finalized there but we are going to go look into that right soal decision okay I was just kind of missed it okay yeah

00:26:00.000 --> 00:26:59.999 align:start position:0%
no yeah I'm not sure if we want to discuss that more because it's not actually nobody's actually written like a a formal this is how we're how we would propose an alternative for it to work I mean I could say something right now that I think aligns with what niik was talking about but I don't know if I think it'd be valuable if we actually get something written down then present it in the future meeting because I know we we have 30 minutes left and dimma has a couple caps to present that's probably more valuable right great all right go ahead okay yeah great yeah I don't actually mind spending more time on the events if necessary but if you're done we can go to to app

00:27:00.000 --> 00:27:59.999 align:start position:0%
that's six hisory so this is basically a cap that mostly changes the transaction set but it does so in a pretty interesting way so as you may or may not know Al a hustle you need to go through with a Footprints inur transaction is done for a good reason and this reason is being able to run the transactions in parallel because if you know your footprint we know that you don't have a data dependency between two transactions and you actually can run them in parallel without worrying about any any synchronization so given the footprints in theory today what you could do is you could right your own version of core application logic

00:28:00.000 --> 00:28:59.999 align:start position:0%
that takes the transactions and partitions them somehow into threads that R par from each other but the issue with that is that there is no good boundary on how much time is it supposed to take for example if you take 10 transactions without data dependencies you could run them in 10 shads and they would for example take 10 milliseconds after synchronization or you could have the same 10 transactions but all of them dependent on some W your entries are being updated and thus you cannot schedule them into 10 different threads you would run them in the single thread fres and in the end you will spend not 10 milliseconds but 100 milliseconds to apply all transactions so basically there is since protocol doesn't do anything currently about pration there is no good way to

00:29:00.000 --> 00:29:59.999 align:start position:0%
schedule the transactions in such a way that it takes some bounded and expected well well there is still an upper bound on the runtime but this upper bound rise wildly and it's not good if Ledger May close both within 100 Mills or within like two seconds for example which is why we are doing this cap fix3 which solves exactly that problem which is given a set of transactions come up with a transaction set data structure that guarantees certain time for appliances transactions is a c of course of that time being time in model constructions and not of course well time CL time because we kind of cannot tell it beforehand without tring the contracts but we hope that our cost models are

00:30:00.000 --> 00:30:59.999 align:start position:0%
good enough and another cave being that you actually should have the fish is sufficient amount of physical course to support multi stading because well again if you have just one core obviously it doesn't matter if you run 10 shreds there some here so if you have just a single core and you wanted to apply transactions in 10 threats obviously you'll not get any performance gain because well transactions are purely CPU bound bound so that's the motivation for this cap and the way it works is that we Define a new structure for the transaction set that instead of having a linear array of

00:31:00.000 --> 00:31:59.999 align:start position:0%
transactions defines two levels of group transactions together and the first level is called stages and I will talk about why stages and what does it need for any moment and then every stage consists of multiple clusters of transactions and within the cluster transactions generally may have data dependencies so it is expect Ed in this cluster to have data dependent transactions that need to run run sequentially however there are no data dependencies between the Clusters themselves and thus since every cluster is guaranteed to be independent of every as cluster what you could do is you could take every cluster and put it into a separate physical thread and thus you can apply the whole stage in parall is as many

00:32:00.000 --> 00:32:59.999 align:start position:0%
threats as you have rers rers also I probably forgot to clarify what we consider data dependency it is when a single transaction has an entry in a retrade footprint and then another transaction has the same entry in either readon or retrade footprint me that one transaction modifies the entry another transaction as read so modifies it in either case we need to sequence them because if you don't do that then we will get non deterministic results for any these transactions now why do we have this additional level of transaction grouping in stages the reason is that if we just try to naively say hey let's just par the put the transactions in separate threads put them transaction set and that's it then you may run into some

00:33:00.000 --> 00:33:59.999 align:start position:0%
issues with C traffic patterns for example example imagine an oracle contract for example right and imagine it updates and entry some key and then there are bunch of transactions that want to read the value from the contract from the ccal contract and that key and thus we have data dependency on a single entry but we have only a single rate of the entry but we have a lot of rats of the exact same entry so what we can do with this stages is that you can put right into one stage and then all reads into different stages stages and basically this introduces just a few barriers of execution into the transaction execution schedule but it

00:34:00.000 --> 00:34:59.999 align:start position:0%
allows kind of efficiently work around this traffic patterns and given smart enough maybe not even smart enough algra for actually coming up with this data structure you can deal with a surprising amount of conflicts all at once without introducing any special scheduling and synchronization procedure so basically just have a few barriers in between the transaction applications and this allows resolving a lot of the conflicts at the same time before driving deeper I go read that for a moment and there any questions far okay the new adjust approach okay so this app itself just changes

00:35:00.000 --> 00:35:59.999 align:start position:0%
the structure of the transaction sets it doesn't have any particular changes around how we which transactions do we pick at all but what I can say that the protocol specification remains such that that transaction ification order is randomized and I think in case of M manipulations nothing changes from the current approach because for any Arbitrage you're probably going to have the data dependency and data dependent transactions are applied sequentially and all we shuffle all the stuff that is being applied sequentially so the transaction order is still for to predict and yeah I don't think it's

00:36:00.000 --> 00:36:59.999 align:start position:0%
any easier to manipulate than it is currently which is to say it's not impossible but just spaming Ledger is a arbitr transaction for example example yeah in terms of the system requirements again the cup doesn't Define them I will will talk about this in a moment but of course yes if we want to run transactions in parallel then well you need to have certain number of course present on the validator which is I guess a tradeoff with the like that's horizontal scaling the notot use better Hardware as then cannot find more

00:37:00.000 --> 00:37:59.999 align:start position:0%
transaction M resarch maybe maybe I think I already have the section on shoing the transactions so may be efficient maybe not yeah okay I see question regarding Z and let me actually go a bit deeper regarding how exactly things are specified first thing is that Moro does introduce a new network configuration setting for the maximum number of f per stage which roughly well not roughly but basically maps to the EXP Ed number of course that your

00:38:00.000 --> 00:38:59.999 align:start position:0%
that the network is willing to throw it when ledgers if you have less course then it may take for you longer than expected to pric transactions you have more cores or at least that many you should be good and as all the network configuration settings this setting will be Modified by validator vot so you know some validators say that we don't have that many cores they will not vote for this and ultimately Network can decide if requirements for to rers are too expensive or too hard to come this come up this and also to make it clear when we upgrade to protocal 23 this setting will be set to just one so any parallelism allowed as network

00:39:00.000 --> 00:39:59.999 align:start position:0%
settings so you know that to enable any parallelism Network quote to happen Okay then to the structure okay I already talked through the specification of the data structure the phas fee and search pricing will work in the same way as it works now which is there will be a single base fee for the transactions it is 100 Stoops if there is no search price in and if there is SAR pricing which means like if there were transactions that you couldn't include in The Ledger but this that were in the mle we take the fee of the cheapest transaction that we do include and use it as a base fees that everyone has to pay

00:40:00.000 --> 00:40:59.999 align:start position:0%
okay and now again to the apply order first there is still a canonical apply order that is defined in cap so you still can apply all the transactions sequentially and arrive at the same result as you would if you were if you applying them in parallel and similar to what we do now we Shuffle the things before applying them first we Shuffle every cluster using the hash of the transaction set and hash of every transaction then the for the Clusters we don't need to shuffle them but by the transaction

00:41:00.000 --> 00:41:59.999 align:start position:0%
hashes and then we Shuffle the stages as well so yeah like so transactions are in different stages one may happen before or after another one one that's kind of what to do for Shing and I think it's not significantly different from our St yeah I don't think it does anything specific to again it doesn't changes status quo for M on the large scale like I think the best thing you can do is still just Spam the network is heritor transactions it is the issue we have now and it

00:42:00.000 --> 00:42:59.999 align:start position:0%
persists like the goal of this cap is not to prevent these types of attacks but it does not make it any easier to guarantee any particular execution order between two transactions so yeah I don't see like if you see something specific like maybe you know maybe we can up Asing later but I yeah I don't think there is any change to to the execution orders that is relevant for for right okay so that's a kind of simpler part of the cap so again multiple threads to apply transactions in parallel oh the most important thing that I forgot to mention is that

00:43:00.000 --> 00:43:59.999 align:start position:0%
we limit the number of sequential instructions across all the stages which means that for every stage we look at the cluster that takes the longest time in terms of model instructions and then we sum up this numbers across all the stages and then we come up with some number of sequential modeled instructions which are hopefully approximating the real runtime for applian the transaction set given and physical threats for example if the Ledger limit currently is 500 milon instructions with this cap for example if you set Ledger Max dependent TX clusters to for example eight meaning you need eight physical threads and we keep the limited 500 million instructions we expect the weder application time be roughly a factor of 500 million virtual instructions which we still on to few hundred milliseconds

00:44:00.000 --> 00:44:59.999 align:start position:0%
currently our cost models models yeah so that's kind of the simpler part of the cap the trickiest thing is the GTL update semantic change so all this data dependency stuff is very good and nice but one thing that we again consciously did when La in San is we allow extending the dtl of the entries even if the entries are in read only footprint and the reason for that is that well we expect TTL updates to be really prevalent and if we treated them as rights there is a big chance we wouldn't be able to AR anything at all because things would be just just Clump together with all this clusters of

00:45:00.000 --> 00:45:59.999 align:start position:0%
for us contract that happen to update totl on the same entes which is why we decided that we actually can reconcile the r changes after running all the transactions without risking introducing any nondeterminism and this TTL updates someand exchange is doing exactly that changes how we update ttls in such a way that changes to TTL done by transactions that only touch the readon entries are not observable until someone actually writes the entry or until we have finished applying all the transactions which is good enough to be able to actually run all the transactions that have the same ke only the run them in parallel and still right have the proper TTL

00:46:00.000 --> 00:46:59.999 align:start position:0%
value and the algorithm proposed is pretty leny but the gist of it is that if two transactions update to TL on the same entry they both will see the initial state of the TTL of the tantry and will be charged the respective fee for set so for example if transaction a has a key in with only footprint and updates TL of some say xlm contract rate by 200 ledgers increases TTL by 200 ledgers and then transaction B increases TTL of exm contract by 100 ledgers well the transaction a will pay for extension by 200 ledgers and transaction B will pay for the D extension by 100 Ledges but

00:47:00.000 --> 00:47:59.999 align:start position:0%
what we will do in background is we will correct this TTL changes and only apply the maximum change out of that to the ler out of this two to the ler which means that the exm contract will be in the end extended by 200 lers and it does introduce bit of annoyance into how mattera should be processed which is unfortunate but I couldn't think of a way that is both compatible with prism and doesn't require any changes in met ingestion and this change is that basically when ingesting the changes in TLS from the mattera you should never decrease TT this is only thing you need to do

00:48:00.000 --> 00:48:59.999 align:start position:0%
it is documented in the cap and you'll probably make another announcement but yeah we kind of get around doing something special for the map and this seems like the minimum possible change so TTL like that you record in the back end and never go down but the TTL That You observe in the is correct DET change for the mattera is correct in the context of a given transaction meaning as I mentioned before that will charge the fee based on the changes that you observe in The The Meta Meta and this is kind of the ju of it and U right yeah I can talk a little bit about candid generation and then move on to question caps don't typically provide

00:49:00.000 --> 00:49:59.999 align:start position:0%
a way of how to actually build a transaction sets which makes sense it doesn't need to be a part of the protocol as long as it is valid it doesn't matter how it has been built but will'll likely implement the simple gitty algorithm described in the cap and the idea behind it is just to pack the transactions into the stages R while uring we are utilizing the stes properly by being packing from time to time and the efficiency of being packing heris sixs is surprisingly good so in the end I think we can I added some benchmarks and we can both P the leeders pretty efficiently like uze most of the instructions and we can also deal with a decent amount of

00:50:00.000 --> 00:50:59.999 align:start position:0%
conflicts like if you have some relatively sparse clusters of conflicts that will not cause any depredation in lure utilization at all and in order to have any real degradation un need like some super inter interdependent transaction for which I don't expect to be happening in the wild so I think this should be good enough at least initially and if we ever observe there are issues with that ways to deal with that both at the mle level and the U wayer of building the transaction sets I think that's all I wanted to talk about just to present the cap and now I will most through the chat and read some questions

00:51:00.000 --> 00:51:59.999 align:start position:0%
 yeah question from niik about the meta changes for TTL TTL M net diffs for ttls not emit one per transaction well we still need to emit TT per transaction I think because they are an effect of the transaction this is how we charge P and I I'm not sure it is correct to just omit them and whether we want to update meta I have

00:52:00.000 --> 00:52:59.999 align:start position:0%
considered this I I'm not sure it is necessarily a better solution so basically the alternative to what is proposed in this app is well we could add yet another field to the mattera that will explicitly output the TTL every entry facted by the wer the final for it after applying every transaction in The Ledger I like that's potentially a significant amount of duplication unless we also kind of remove the changes for transaction I'm not sure if it is a good idea to remove the changes per transaction because then well meta just has some entries drop that transaction has actually affected which makes tricky to debu transactions for example makes to understand your fees if you ever wanted to run the computation logic

00:53:00.000 --> 00:53:59.999 align:start position:0%
comp so I'm sure we can omit that Pro transaction changes but then I'm not sure if duplicating this data is necessarily making things much easier to just not decrease in TTL with your track on on it it so that's my opinion and I mean you either need special Logic for merges or you need special Logic Logic for this new types of TT entries and you also need to kind of sometimes ignore that for transaction changes I'm honestly not convinced if it is a good idea and if you wanted it possible to only ingest the T changes after the LED then so that you don't even need to process per transaction changes at all

00:54:00.000 --> 00:54:59.999 align:start position:0%
then basically we duplicate every TTL change into which is again probably not very Fe you don't know n if you want to talk more about this or we can continue offline okay I don't see any more questions and we actually have just three minutes left so if anyone has any questions please feel free to comment on the ad discussions read it is yeah I guess the to meta change is really the most controversial thing about it everything else

00:55:00.000 --> 00:55:59.999 align:start position:0%
is really uncontroversial I feel because though we are just allowing to include more transactions in Ledger and we still shuffle them so no changes for any so yeah yeah and one final thing I'll say thanks thanks for presenting Dima We we forgot to talk about back to the unified events a one topic about how the sa events have the asset name of the topics but SE 41 doesn't specify having putting the asset name in the topic so lee is about to start a thread about this so we can discuss it there yeah yeah Le just posted the thread in the chat yeah that's all we have for today thank you for joining