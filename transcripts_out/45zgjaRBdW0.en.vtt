WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:59.999 align:start position:0%
so Darth do you want to give us a quick overview on the scalable configuration issue yeah I can do that can you hear me yes I can hear you okay perfect so yeah the issue at the moment is that all current Network related settings are stored in The Ledger header and this isn't a good place to store you know the sorbon settings because they can be quite large I think the current estimate for meeting or alone sorry for metering alone is when kilobyte so before we go through the approaches I'll mention that they all assume the use of dedicated Ledger entries for storing settings as described in cap 46-9 and that the settings will be created on protocol upgrades the only differences in these approaches are how the settings themselves are upgraded so approach one which is described in cap 46-9 this requires a mechanism for validators to introduce and distribute setting upgrades directly to the overlay the validators will then vote on the hash of the flooded settings

00:01:00.000 --> 00:01:59.999 align:start position:0%
approach 2 allows users to use sorbon to introduce possible setting upgrades into the system using arbitrary Ledger entries allowing validators to vote on one of these entries to overwrite an existing setting we use sorbon here so accurate fees will be charged for the large for the larger operation and Ledger entries that will be introduced there's still a question of how the user will interact with sore button we can add a new operation add a new option to the invoke host function up or rely on a smart contract itself to create these entries approach three is similar to approach two except that a new Stellar classic operation will be used to create the entries that validators will vote to use to make this Opera to make sure this operation isn't spanned the idea here is to make sure the to make the operation withdraw a large amount of excellent from The Source account but this idea needs some additional work to guarantee that the operation always has a sufficient balance for the withdrawal once the operation has been flooded because you don't want the

00:02:00.000 --> 00:02:59.999 align:start position:0%
operation to get flooded and fail because the network will have already flooded the that doesn't work for the operation operation so yeah that's the summary of the three approaches and I know their their opinions on and I know their opinions on all these so the yeah this is Homer do you have a do you have any opinions on what would you prefer Sid I have a question so can you tell us a little bit more about what is stored in that in these Network configurations like concretely and why is it important to preserve these that data so for example from you know for

00:03:00.000 --> 00:03:59.999 align:start position:0%
metering we need we have different parameters on how specific things get charged charged and so the structure will have you know say like you know a bunch of u64s u64s that that indicate how like specific settings should be charged and that that settings should be charged and that is what sorbonne will use when deciding how metering should be used but we want these settings to be configurable that's why it's not just hard-coded in soroban the settings will be stored somewhere and which and the value should be able to upgrade the settings whenever they want if all the validators agree does that make sense yes yes it just it's it doesn't sound like something that would change frequently between two subsequent Legends right but it's more something

00:04:00.000 --> 00:04:59.999 align:start position:0%
like yeah when we'll go to upgrade to protocol Version Y then we need to make that update am I cool we don't want so we don't want to tie the upgrading of the settings to protocol upgrades right for example right now we can update update I believe like at the Base reserve if we want to without updating the protocol right so that we want this to work the same way and well I agree that we we won't update and well I agree that we won't update the settings very often it I wouldn't be surprised if we you know did it twice in this specific protocol version it's just generalizing what it's just generalizing what we're already doing a letter header to a much larger number of number like this it's in a potentially unbounded number of numbers although it's not of numbers although it's not like enormous it's not thousands and thousands of entries but it's probably tens or or possibly as much as 100 so tens or possibly as much as 100 so yeah does it make sense to do something

00:05:00.000 --> 00:05:59.999 align:start position:0%
in the like of in the like of having an optional field in the Ledger that would have that data and in any subsequent Ledger header just say hey in the in Ledger number XYZ we've last updated these parameters so I mean that's how I think we're describing is how like current upgrades currently work like you just you submit the upgrade to upgrade specific setting the issue here is that the the settings can be here is that the settings can be quite large right so it's not it doesn't like you know sending a one kilowatt xdr blob in the current mechanism wouldn't work it actually wouldn't work at all next in the with the current xcr because we have a limit of I think 128 bytes per upgrade but which is why we're exploring like a more scalable solution

00:06:00.000 --> 00:06:59.999 align:start position:0%
Ed can I ask you a question about Ed can I ask you a question about the other approaches that that are the other approaches that are not approach number one here in approach number two for example is this is you you sort of you've got is you sort of you've got two things that are in play one of them is sort of propagating promulgating the the proposed changes the set of options and one of them is is and one of them is a consensus vote if people vote on two different config settings you know what happens in approach to like there's still a consensus conflict resolution problem the same as you would have in the first one where there's this arbitrary you know take the largest upgrade choice that the consensus layer is doing in approach two what does a consensus player do if there's two con two conflicting votes it would be the same as what it is today right like if you if two if there's two different votes on say the Base reserve I don't actually know what I don't actually know what conflict resolution is on the Base

00:07:00.000 --> 00:07:59.999 align:start position:0%
reserve right now my point is that it is currently happening in the consensus layer and it approach too it sounds like you're trying to take it out of the consensus layer so then oh I want to say that all the approaches actually are the same in terms of voting like in any case what leaders would vote on the hash of the upgrade set of the config upgrade set and the reason for that is that well we don't want to blow the SCP values as well well because you have plenty of them and even the update upgrades are rare event we don't really want to have huge spines during this forever so voting always happens on the hash of the config upgrade set and the default mechanism for consensus is that if you don't agree this is upgrade evolved against it and then there is not an upholds yes yeah but so all of this happens very quickly all this happens very quickly during the voting round and

00:08:00.000 --> 00:08:59.999 align:start position:0%
I'm not concerned with the the I'm not concerned with the like I object to this vote like I object to this vote because that that's not likely to happen very often unless someone just you know doesn't schedule their validator it's it's the combining thing if if validator it's the combining thing if validator a uses you know arms upgrade for upgrade set a and and validator B for upgrade set a and validator B for B then when someone hears those two or or nominator here's those two or whatever they do a two or whatever they do a a value combined right they do a SCP value combined yeah this is this a not combinable right like that okay you need a vote and for the contract upgrades is a hash of the whole set of upgrades that need to be upgraded atomically like so any conflict just fails immediately any conflict just goes immediately as we don't try like in option one I guess it is possible to do something weird there or you can you know have some

00:09:00.000 --> 00:09:59.999 align:start position:0%
fuzzy matching of the upgrades you know you say like this operation should take thousand units of gas and I say it should take a thousand one unit of gas and maybe it doesn't matter in sand but it kind of introduces too much unnecessary unnecessary surface for potential issues and bugs and sure I'm just saying that looking at the document that I'm reading right here it says that the cap picks the largest upgrades which is arbitrary and requires some sort of conflict resolution it may want to only vote for a specific hash so that sounds to me like a conflict resolution mechanism but I'm just trying to clarify whether that that's that's real or whether it's yeah maybe that's not one or yeah maybe that's how it is but I think in all approaches idea for voting is the same like revolt on the hash of the whole upgrade set and Valderas have two agree on this hash exactly and then the question is normally how we distribute the premature of this hash instead I do have a question on kind

00:10:00.000 --> 00:10:59.999 align:start position:0%
of like the implementation burden like what is between the different approaches what is you know what's the low hanging fruit and what's what requires like additional mechanics I would so I would say approach to should be the simplest correct me if I'm wrong but it shouldn't be hard to you know add a function sore bond to you know create these entries and then you know when the like any mechanism to interact with that function should be simple either a new operation or the invo coast function up or I think using smart contract is a little more complex because I think there's some some there there are more details some there are more details around that but I don't think like I think in my opinion approach to the simplest approach approach three simplest approach three yeah I think we have to be careful and to make sure that it's not spammable

00:11:00.000 --> 00:11:59.999 align:start position:0%
an approach one requires work in the overlay in the consensus layer and is the reason approach to is the simplest because it used it reuses the metering mechanics that we have in in metering mechanics that we have in soroban soroban yeah that's that's the only reason we're yeah that's the only reason we're putting it in in sorbon itself so what does that I would say I mean so what does that I would say I mean approach one is not hard either in the series to protect care for that and they both approaches I don't think there is too much work anyways this one problem is there for surabana too like we figured it out discussing this concerns for example you know by the transactions it install vasms they can be pretty big too and we don't want to flood them for free basically and I guess in this sense I like it's maybe not necessarily a very easy problem to solve but we need to solve it anyways so kind of option to just piggybacks on

00:12:00.000 --> 00:12:59.999 align:start position:0%
this file with options three I guess we need to change something in classic mechanisms to make appropriately so yeah I kind of from drinks it probably too is the best it's a long time for the week week and use the amount of work additional work required yeah could you speak to the could you speak to the complexity of the consensus change that's described in in verse one it says that's described in verse one it says the consensus changes it would be complex is that like because it involves the item fetcher because we would have to have a second like like the way we do DX pitching is that it yeah I mean it's again I that it yeah I mean it's again I haven't traveled I haven't triven this point about consensus layer complexity it doesn't it's complex per se just literally reuses the TX set logic but the except logic is arguably not very simple either yeah replicating replicating yeah I mean on one hand it's trivial

00:13:00.000 --> 00:13:59.999 align:start position:0%
that that we copy something that exists but we are copying something complex so maybe it's really better to just use normal Ledger mechanisms to put this interest into lecture and yeah not bother about I didn't yet another flooding problem the one question I have about that is that that the consensus module has to actually be able to judge when it has a given hash right if there's a vote hash right if there's a vote for hash X it has to then ask The Ledger hey the hash X actually exist because I'm not going to vote on a config setting that I don't have right like in The Ledger right so this is just moving it from moving that question from item fetcher over to a ledger inquiry and that's a synchronous Ledger inquiry in the middle of consensus right yes okay I have no idea if this is a big problem or not hopefully are we okay with that I mean that's kind of the risk of my mind but maybe the maybe that's okay I

00:14:00.000 --> 00:14:59.999 align:start position:0%
think that's probably less pain yeah yeah yeah the thing that is kind of annoying I mentioned that in the annoying I mentioned that in the dock with the approach that relies on on consensus is approach that relies on consensus is that that we would have to to to secure the we would have to to secure the upgrades in some way right now upgrades because they are small we don't actually actually they are not actually signed so you know values in SCP they are actually signed so that you you if some valid signed so that you if some valid data all goes Rogue you can basically blame you know who introduced that the value that is kind of spamming the validators validators so you can basically decide okay I'm going to remove this validator from my Quorum set that's kind of the idea with the signature upgrades are not part of this because it's today they are small so if we

00:15:00.000 --> 00:15:59.999 align:start position:0%
start to make to have to and the the start to make to have to and the reason for upgrades not being included in the sign payload is the there's an option for validator to remove the upgrade from the value if they don't agree with it so for you know so that basically you can still close ledgers ledgers you know with transactions even if there is no consensus on the there is no consensus on the actual upgrades which yeah I mean we didn't have you know so much of those problems you know until now there's like you know broad agreement on on like for know broad agreement on like for example if you want to upgrade the network right to a specific protocol version there was no contention I think we'll have potentially more contention in the future as we think of network parameters that maybe are impacting certain contracts right like as every time we change like fees I mean like

00:16:00.000 --> 00:16:59.999 align:start position:0%
the yeah metering schedule right like we're going to cause maybe certain contracts to become more expensive therefore there is maybe a higher chance of disagreement between between very of disagreement between very data so I think this the the chance data so I think this the chance of conflict is higher in you know post-organ post-organ and and that's why yeah I think we may have yeah multiplications where somebody there are going to say Hey you know I'm I don't agree with this thing so I'm going to drop that upgrade and and then until the conflict is resolved you don't want to be in a situation where the network is not agreeing on basically anything right anyway so so that's kind of why you can anyway so that's kind of why you can drop those those upgrades and drop those upgrades and and right now they are not signed so we would have to either make them signed

00:17:00.000 --> 00:17:59.999 align:start position:0%
if we want to make them bigger or just not you know keep them small and you know that's kind of the option too hopefully that makes sense to people so I do have a question that relates to the the user experience of of changing the user experience of changing the configuration right now you know validators you know just have like Stellar core Commands to control this is the idea to maintain the same experience across all of these different approaches or do different approaches mandate different user experience I think I can answer that I mean it's easier to arm an upgrade for a hash like it doesn't require any changes because hash is a small like if you needed to distribute the whole upgrade set as in option one and that's durable but it's annoying like it's annoying to do this

00:18:00.000 --> 00:18:59.999 align:start position:0%
as a or command you need to maybe extend the command interface to take a file instead of you know just to get request or something like this so approaches like two and following a SIM for instance because it doesn't require any exchanges like we say Hey you like you want to upgrade to this set of configs with hash X here is a link to the lectures that actually contains this entry so I think the distribution is kind of linear in this it's easier to arm these upgrades but it introduces the transactions that needs to be submitted yes someone needs to submit a transaction this is true yeah so the US does change so in all approaches right because in the approach two and three you need to you know submit this operation but in approach one the the initial set of upgrades one the initial set of upgrades needs to be distributed as well right

00:19:00.000 --> 00:19:59.999 align:start position:0%
I guess that would be maybe another solid core command that the validator would would submit and then that that's how would submit and then that that's how that would be flooded right but to be honest I do feel like approach one is honest I do feel like approach one is an extension of the current experience we have which is you know the validators will coordinate the validators always need to coordinate regardless outside of the network right so you know they coordinate on a Channel right now you know Justin tells people hey if you want to upgrade or to change this value you this is the command you use right so maybe the the maybe the mechanics are going to be a bit different we're going to say hey this is you know an xdr take a look at it at a stellar lab or SDC or whatever and this is the hash vote on it so it's still like you you don't so it's still like you don't introduce like another step of of you know submitting transactions to the network well only one person needs to submit the

00:20:00.000 --> 00:20:59.999 align:start position:0%
transaction it's not like value there was once you know the thing is populated you know it's basically like today yeah basically whoever initiates the upgrade should send the transaction which I don't think is a huge View because it's a big thing they need to propagate any weightages foreign got it okay that makes sense yeah by the way this closes a kind of a a potential way this closes a kind of a potential you know communication problem that we have today right like today when we for example when you know we as SDF say Hey you know we are proposing to upgrade the network to you know version 19 right like we did last year people look at that message is like it it and it's not signed we don't publish that with you know digital signatures or anything so in theory you

00:21:00.000 --> 00:21:59.999 align:start position:0%
could imagine somebody faking you know us and then getting people to vote for something else I think the chance of that again increases as you increase the complexity of those things that people are voting on and the benefit of the having the The Entity that is kind of championing for a specific change is that because this is actually an actual transaction on the network it's it's also signed by The Entity so that's kind of you know one of the extra extra pre-free benefit from option two and three three so essentially a configuration change proposals become on-chain activity yeah and actually so that's something I wanted to kind of maybe because we didn't talk about that too much like in in the doc we only kind of mention it that option two like one of the things I think that this opens the possibility for

00:22:00.000 --> 00:22:59.999 align:start position:0%
in the future to have a like a point the validators to to like like a point the validators to like Ledger entries that are controlled by those those so that you can form many validators there's well there is a concept that we discussed before right that is actually in a paper that that was published a few years ago about the notion of governing and non-governing validators and I think this for certain things this will this may actually become like like more interesting you know in the context of yeah Dell's that are not the tier one validators that are kind of managing certain aspects certain parameters that you know of the network and then the the validators

00:23:00.000 --> 00:23:59.999 align:start position:0%
and then the the validators that are interested in actively participating this of course you know they get to they would not just blindly vote for those things but otherwise otherwise other validators just might do that right for for those do that right for those like maybe not as as coupled to to network as coupled to to network operators right or maybe even need like additional voting or something right and you want that to to happen on chain and you want that to happen on chain because you want more transparency or something right got it so yeah it does sound like there's some added benefits to approach too and it seems like from an implementation perspective it's it's fairly simple do we have any it's fairly simple do we have any concern about you know it's not immediately you know it's not immediately obvious that Stellar configuration changes should be proposed on soroban it feels a bit like we're using just

00:24:00.000 --> 00:24:59.999 align:start position:0%
sorbon because it's it's there sorbon because it's there and you know I don't find it and you know I don't find it offensive but but like can there be offensive but like can there be you know issues from it and things like we want to do like in pure stellar and and now we're introducing them to the storybond I think this kind of ties into like subtopic I wanted to talk about is whether or not to be as separating sort of an operations into like different operations in the transaction because currently you have just one golden vocalist function that does a bunch of stuff and this changes it would do even more stuff so I'd say using suraban is just an implementation detail here and nothing prevents wake up an option two from being a separate operation and also we may want to do this to all the operations anyway just to you know have a single flat hierarchy of things you can do to discover

00:25:00.000 --> 00:25:59.999 align:start position:0%
which basically would make it pretty opaque in terms of like how exactly transactions have been executed I think improves the the X sound would I think improves the X sound would so I can so the derived question here is whether we want to split the invoke host operation into like multiple types of cerebon invoking operations yeah right like in this particular case like I feel like it's a race a strong case for making it a separate operation it was very explicitly namely containing internal assumptions that you know people don't randomly submitted and for all the existing operations there is the same consideration I know people have any opinions on this would be accessible from from the story would be accessible from the story about environment in addition to being

00:26:00.000 --> 00:26:59.999 align:start position:0%
accessible through the this dedicated operation operation no I mean the same about the current no I mean the same about the current invoke host function transaction operations that annoys me and which is why I'm proposing this is that we are not actually invoking a host function in the same way you can invoke it on yeah from from the contract like they are Divergent they like the cost functions that we invoke from Stellar core are not the same course functions that are more invoked from contracts so the name doesn't make too much sense I mean even though initial intent kind of made sense but is there any implementation and requirements ended up they're not quite consistent and the the convinced we should keep it this is in this cost functional you know we could just be a bit more explicit and just say the operations and the adapter lessons you are doing without involved in the Waterhouse at all all yeah my opinion

00:27:00.000 --> 00:27:59.999 align:start position:0%
yeah I think the original the yeah I think the original intent was to make calls original intent was to make calls uniform and I think we may be paying a fairly high price to try to make two entry paths to call look the same you know we're sort of orienting everything in order to make that one path prior to use code well I think maybe like the this is more related to I saw there was like if we have a dedicated host function for all this stuff I agree maybe we need to maybe we need to think about this but like I I'm not actually convinced that this is the necessary to have anything first class for for those Network first class for those Network upgrades inside suraban like I think we should be able to point to basically any saruban State should be considered valid for like as long as

00:28:00.000 --> 00:28:59.999 align:start position:0%
we can you know like I was saying in the dock like it's basically a byte you know a byte array that we happen to be able to decode you know using because it's some xdr right that we can understand that the network level but that in from a Solomon point of view I think it's just a byte array array and if we do it like that there's nothing special about like any of that stuff okay this is an important point that I didn't understand so from the approach number to your perspective we're not actually introducing changes to soroban in order to facilitate this so what is in the dock right now it talks about having special Ledger entries that are like you know so you need like a host function to basically manage to the special entries but I'm not at you know when we originally discussed that I didn't think we would

00:29:00.000 --> 00:29:59.999 align:start position:0%
actually have dedicated legendaries for the for as input to to to the upgrade I thought that at the end yes you know when the when upgrades are actually when those network settings are actually active yes there are special entries on Ledger but in terms of like how do we how do you get did they get fed into the upgrade function that I don't think we I don't think we need to have a dedicated dedicated we need to have a dedicated Ledger entries you know with that specific format as input would that be at that point in the dark was more to specify that it's not a config setting entry entry I guess it wasn't you know it wasn't clear what it actually is because I wasn't expected out but what Nico's saying is probably the best way to do this where we specify that you know for an upgrade it'd be a byte array of a config setting entry or a byte array of a vector of config segment entries right

00:30:00.000 --> 00:30:59.999 align:start position:0%
I'm not sure I'm good at the point because they don't have an entry like sent to me it would just be I think with Nico's things it'd just be a contract data entry it's not a contract we don't have a contract so it cannot be contract data data you know you can no like is anybody like you know as part of this right like you can imagine that's why I think like as if you think of if you think of in the future you want to have some doubts doing that like it's just a special case like like if I want to special case like if I want to in the first iteration of this the way you would do it is you have a dummy contract you know that has like State you know associated with it that is just a single you know that has a single value in it and that allows you to to basically it and that allows you to basically you know as a user I can persist I can yes like kind of like a balance right

00:31:00.000 --> 00:31:59.999 align:start position:0%
for for you know for token contract but instead of a balance it's basically a binary blob and then in the upgrade that's how you get it on The Ledger all right and then the upgrade just points to that specific Ledger entry that contains the binary problem of Interest so what do you guys suggesting we would have a contract specific cost function no there's nothing specific to this it's it's just like yeah to in order to construct how how would we vote and to construct how would we vote and wait like it has to be special right like a similar proposal and option for opposite opposite there needs to be a way like since we are voting on a hash or something you need a straightforward way of finding this hash in The Ledger yeah that's why you you can point to any you can point to any arbitrary contract data right you're using the Ledger key

00:32:00.000 --> 00:32:59.999 align:start position:0%
a hash yeah you bought an electric key you would vote on a pair Ledger key hash probably right like maybe you can make it work with just a ledger key but like that I you know for now I think it's that I you know for now I think it's still a joke right right right maybe we need to yeah we need to maybe we need to yeah we need to maybe like sketch that a little bit so that I can so basically you would vote on say I mean that's quite inefficient right right I guess the best you can do is contract 80 plus some data ID and accurate format actually I do but so I think this has been super informative it sounds like there's a very strong bias towards approach number two here and there are some details that we need to figure out like whether this is actually something that's special cased in sorobon or it's just a a general

00:33:00.000 --> 00:33:59.999 align:start position:0%
in sorobon or it's just a general ledger entry and then the validators is part of the upgrade process just pointing at this Ledger team so let's I think we can take this conversation offline about the details and and if needed we can bring and if needed we can bring these up in one of the next meetings we have around 20 minutes left and so before we go on that Siddharth do you think we have enough to to keep prototyping here yeah we should be good okay okay so we don't have a lot of time we have 20 minutes and Garand is here today to start talking about State expiration which is a very big hairy topic so garen's gonna give a very brief overview about this today if we have time we're going to do q a I imagine that we're going to be talking about these about this specific issue a few more times so no big decisions will be made today during the stages

00:34:00.000 --> 00:34:59.999 align:start position:0%
yeah so I guess we want the time in time so I just wanted to kind of start out with the general motivation and then talk about more about the interface and then we'll leave the implementation details to probably future conversations so essentially the issue we're trying to solve with archival state is this issue of unbounded Ledger State size at least right now it's not a classic the number of Ledger entries we have to store is growing and there's not a strong incentive for users to delete entries additionally a large amount of those entries that exist on The Ledger are either outdated or just won't ever be used again for instance we have a lot of climbable balances that are more spam-like entries that aren't used very often or won't be used at all they're taking up this Ledger space and increasing larger bloat and so for the healthy skill patterns of the network we want to be able to essentially cap Ledger State size and not allow for this arbitrary growth and to do that we want to essentially delete entries that

00:35:00.000 --> 00:35:59.999 align:start position:0%
aren't being used but somehow not delete and keep the entries that are being used often and so in order to do that what we do is we want to implement this concept of rent so essentially for all sorbon smart contract data entries the data entry will have a rent balance which is some amount of xlm that is reserved for that entry to pay a rent fee and then as that entry lives on The Ledger it will have a rent fee deducted from that rent balance and then whenever that entries rent balance goes to zero the entry is deleted from The Ledger answers if it never existed before now this is obviously it opens up some issues right so imagine you have a wallet or a balance that stores a lot of tokens that are valuable you wouldn't want this entry to be permanently deleted and lost just because you forgot to pay rent and forgot to up to rent balance and so instead of even though the entry is permanently deleted from The Bucket List what we do is we take that entry and we send it to a special

00:36:00.000 --> 00:36:59.999 align:start position:0%
kind of node called an archiver node and this archiver node is essentially storing all of these entries and then if a user then wants to use an entry that has defaulted on rent and then been sent to an archiver node then what they have to do is they have to go pay a fee and then retrieve that entry from the archive node and then give that entry back to the validators and then once that fee has been paid the validators will then take that archive entry put it back on the bucket list and then this entry can be used as if it was never deleted deleted and so kind of the implementation details as to how this work is the archive is implemented in a Merkle tree-like structure and then in order to restore an entry from the archive and then put it back on the ledger so it can be used again you have to provide a proof of inclusion that this entry that you say was archived actually does exist in the archive and so this Merkle structure is very powerful because it means that validators are able to check and make sure that the entry you say is in the archive is actually legitimate

00:37:00.000 --> 00:37:59.999 align:start position:0%
but the validators don't need to store the archive all they need to do is store this root Merkel hash and then they are able to validate proofs that are generated from the archive so kind of high level how this works is the validators themselves don't store any of the archive State and cannot produce these proofs but the archive of the archive nodes do store the entire archive State and have enough information to produce these proofs and so the archive nodes we kind of Envision to serve a similar purposes kind of horizon where they are supporting the validators but are not directly involved in consensus and so from an interface perspective this has a couple of issues that we kind of need to discuss especially when it comes to security and so because the we want the validators to store as little State as this archive is possible they don't store any of the keys and they don't know what's in or not in the archive and so if you can imagine an example say we have like this token

00:38:00.000 --> 00:38:59.999 align:start position:0%
contract right and you have a balance on that token contract but you don't use it and So eventually the rent balance on that entry will go to zero and that entry will be deleted from The Ledger and then stored in the archive now once you default on rent from the perspective of the validator it's as if this entry has never existed before and because the validator doesn't actually store the archive it has no way of knowing that this entry used to be in the bucket list or that this entry is currently in the archive and so this means that if say the smart contract was to create another balance with that exact same address this would be allowed right because the archive or because the validator doesn't know that this entry and this key exists in the archive it will create and generate new keys which means that we have this issue of key collisions where you can imagine if this process was repeated several times it's possible to have an entry with multiple different versions of that entry with the exact same key that exists I'm simultaneously on The Ledger and also in the archive

00:39:00.000 --> 00:39:59.999 align:start position:0%
now for some types of data these key collisions aren't that big of a deal right so if you can imagine if you have say a balance of some amount of token with your given key and there are multiple different balances you have so say you have 10 xlm in an account and then that 10xl imbalance gets archived and then someone sends you another 5xlm and eventually that entry gets archived with your key there are two entries in the archive that corresponds your balance one with 10 xlm one with 5xlm but this isn't that big of a deal because both of them are valid right you can just restore the 5x on them balance spend that 5x Alum and then restore the 10 xlm spend that no problem because both of those balances are valid even though they have the same key but for some data types that's not the case right so if you can imagine a smart contract implementation that uses a nonce value so let's say that we have something like usdc that uses a nonce to protect against double spends and make

00:40:00.000 --> 00:40:59.999 align:start position:0%
sure that transactions can't be played say that this nonce value is something like 10 a non-zero value and then gets archived well now this token contract when it needs to do something the token contract will see that there does not exist in nonce because from the perspective of the validators the contract itself has no way of knowing what's in the archive because the contract is running on a validator and the validation store the archive and so whenever the contract sees that there doesn't exist a nonsense on The Ledger instead of going to the archive and restoring the nonce with a correct value of 10 what the contract will do is just create a new nonce with a value of zero and so what you can imagine is say that we have the correct notes value which is 10 in the archive and this new zero nonce because this nonce is incorrect and is essentially shadowing a valid version of the nonce in the archive this allows malicious users to execute a replay attack because they can take a

00:41:00.000 --> 00:41:59.999 align:start position:0%
transaction that says hey if nonce is equal to zero this transaction is okay and even though the correct non's value in the archive is 10 the knots value on The Ledger is zero and so they can replay this transaction and maliciously use this nonce value and so we have this challenging problem that for certain types of values like nonsense there needs to make sure that you only have a single version of that entry between both the bucket list and the archive this is a problem because the validators don't store the archive and so there's no way of checking if a key exists or not directly from the archive so that's issue number one this thing like nonsense where you want to make sure there's only one version of the entry that exists and then the second or the second issue that comes up with security is something where you restore an outdated version of an entry so similarly let's think of a token where you have some sort of kyc Entry right and so say that you have a kyc

00:42:00.000 --> 00:42:59.999 align:start position:0%
entry that allows a user to spend their tokens and then that kyc entry isn't touched for a while and So eventually it runs out rent balance and gets sent to the archive now a new kyc entry is generated on the bucket list but this kyc entry revokes access to those funds now let's say that this new kyc entry that revokes access isn't used for a while and it too falls into the archive but a malicious user might do is go into the archive instead of restoring the most recent version that revokes access instead restore the earlier version of that kyc entry that gives the user access to spend those funds and so by restoring an out-of-date entry what you can do is essentially do a versioning attack where you take an out of date entry restored onto the bucket list and then you're essentially able to pretend as if it is newer than the kyc entry that revoked access and so essentially we have two issues we need to solve here first we need to make sure that you can only restore the

00:43:00.000 --> 00:43:59.999 align:start position:0%
latest version of an entry so you can't do this like kyc rollback attack and second for certain types of data we need to make sure that there's only one version that exists in the archive in the bucket list now we don't need that guarantee of the uniqueness guarantee for all types of data for example the balance example I talked about earlier it's completely fine if you have multiple different balances but for things like nonsense you need to make sure you only have one and so essentially because we have these two different requirements we expose two different types of storage at the sorbon interface level right now we're calling this unique storage and recreatable storage now essentially the differences here is that unique storage guarantees that whatever entry you have there's only ever one copy of so in our non's example if you're using unique storage and you say you know I use unique storage to create announce value and that Knox value gets sent to the archive if you try to recreate that value the function will panic because it says hey this is unique storage and entry already

00:44:00.000 --> 00:44:59.999 align:start position:0%
exists in the archive so I'm not going to let you to recreate that under the hood how we do that and again this is an implementation detail for later is we use a combination of proofs of inclusion and also proofs of exclusion so for instance whenever you create a unique data entry for the first time you need to also provide a proof that this entry has never existed before and so this proof needs to become from the archive nodes because the validators don't store enough information to prove that something never existed but if you provide this proof to the validators they are able to check the proof and make sure it's legitimate and so because you have this extra step for Unique data and needing to prove that nature never existed it's more expensive now the recreatable data doesn't have this guarantee and so for a thing for entries like nonsense that have security implementation implications you wouldn't want to use recreatable data because you could have multiple versions of your nonsense and you could recreate nonsense and have security issues however you would want to use

00:45:00.000 --> 00:45:59.999 align:start position:0%
recreatable data for something like a balance that doesn't have this issue because recreatable data is much cheaper because you don't need the security guarantees you don't need to provide all these proofs of exclusion and whenever you create a recreatable entry you don't need to prove that's never existed before before and so essentially we tried to provide these two classes of data so that users who need some sort of security guarantee can use the more expensive and slower unique data but for entries that don't require those strict guarantees you can use recreatable data now in addition to these two data types we also introduce a third storage type called temporary storage and what temporary storage is is it kind of just temporary storage is it kind of just does what's the name sounds like it's entries that are meant to be temporary and so essentially if you have a temporary storage entry whenever its rent balance goes to zero instead of being sent to an archive it is permanently deleted and because you don't need to worry about sending this to the archive and there's not this archival cost the temporary entries are

00:46:00.000 --> 00:46:59.999 align:start position:0%
the least expensive storage type now you wouldn't want to use this for sensitive data such as balances because if a balance defaulted on rent it would then be lost forever but for temporary entries such as if you want to give another user authority to use your funds for say 100 days you could use a temporary entry that automatically deletes itself or if you had some sort of data that is easily recreatable such as like a payment Channel you could also use temporary data for that because whenever it was deleted you could just regenerate the exact same entry again and so these are the three data types in the storage mediums we're trying to present to the end user now the issue is we have these security issues that are generated from the archive interface and they can be very tricky to protect against for instance it's a very tricky to think about examples like kyc rollback and nonce attacks in the context of this archive especially since archival state is not only a new interface for Stellar but a pretty new interface for blockchains in general and

00:47:00.000 --> 00:47:59.999 align:start position:0%
so we want to provide an interface and Define use cases as clearly as possible for each type of storage and try to make this as seamless as possible for the end developer and trying to abstract away as much of the complexity from the archival interface as possible so I think this is a pretty tricky problem problem but I think that's the high level issue I want to talk about today is just the security issues that arise from this archival state and then the three types of storage that we've tried to implement so far and so I think I'd be open to any questions or if anyone has anything I'd like to talk a little bit more in depth about about bro bro thanks for that just a quick question you just you were talking about unique storage and having to supply proofs that does that mean that in order to create a unique storage entry I need to have access to an archival node node yeah so essentially how that works is out when you actually from the

00:48:00.000 --> 00:48:59.999 align:start position:0%
perspective of the validator itself so yes you do need to have access to a an archiver node to produce these and so kind of what we're Imagining the interface would look like is that RPC nodes nodes kind of double as pre-flight nodes and archiver nodes and so during the free PreFlight process if you generate a unique entry or if you want to access an entry that's in the archive of the PreFlight note itself will carry archival data and so the PreFlight note will be able to essentially kind of work similarly to the footprint when it generates Footprints for a read write data accesses that the pre-flight node will be able to retrieve the proofs that are required for your transaction and then give those Trend or give those proofs to the validator and so because the PreFlight node is generating and providing these proofs whenever you actually go to apply that transaction on the validator the proofs are available because they've already been made in advance by the previous flight node

00:49:00.000 --> 00:49:59.999 align:start position:0%
and just a bit here for recreatable storage I don't need that right yeah so for recreable storage it's cheaper because you don't need to provide any proofs and there's no work to be done you can just if a key doesn't exist on The Bucket List you can just generate that new key without any checks or balances no problem well to make an amendment like it's not like nothing has to be done through a created both storage because if you want to benefit from the fact that it's recreatable you need to write some codes that allows you to nurse entries for example if you have a token contract and your balances are creatable you probably want to provide a contract function that allows users to recreate there is a balance given approve basically so like both involve something to be done but in case of unique storage it would need to be done every time you create a new entry while for a tradeable

00:50:00.000 --> 00:50:59.999 align:start position:0%
it's only if your entry has got archived and you actually want to restore it which hopefully shouldn't be the case too frequently yeah kind of the difference is in unique storage if something exists on the archive you must restore it in recreatable storage if the entry exists in the archive you have the option of either restoring it or just if the restore would be too expensive or you don't want to restore it for some reason just creating a new entry with the same key key but but just to be clear with regards to what Dima just said if I do have an existing if the Ledger entry has been recreated with the same key when I try to restore it right now will it fail or will it like use some sort of like custom merge capability what's what's gonna happen yeah so when it comes to recreatable data there's a function that we Define called an archive latest version and

00:51:00.000 --> 00:51:59.999 align:start position:0%
essentially how this works is whenever you restore unique data it's automatically automatically add to the bucket list immediately because unique or for Unique data because unique data is known to be unique and so you know you won't have a key collision between the archive and the bucket list so you can just automatically add to the bucket list for recreatable storage this isn't true there could be a key collision and so whenever you restore a recreatable storage entry it's not immediately add to the bucket list but instead this function restore latest archived entry just Returns the unarchived version right and so in the doc if you look at the last two pages there are two example functions on how to restore our recreatable data to example implementations that contract might do but essentially because of these key collisions it's the responsibility of the smart contract implementation to resolve these collisions and so for instance what you can do is after you get the restore

00:52:00.000 --> 00:52:59.999 align:start position:0%
latest or the unarchived entry returned from this function you can check the bucket list and see if there's a collision and then if there's no Collision the smart contract can just write the key immediately no problem but if there is a collision then what the smart contract might do is compare the values of the recently unarchived version and the bucket list version and pick one or discard the other or in the example of balances there could be some sort of merge operation defined by the smart contract and so if you have two balances the unarchived behavior might be okay take the unarchived version sum the balances of the version that's live on the bucket list and the recently unarchived version and then just write the result which is the sum of those two balances and so because there are different use cases depending on the contract that's a contract implementation that this contract developer itself might think must think of it and is there a I see you have a section on default implementation what should be the default

00:53:00.000 --> 00:53:59.999 align:start position:0%
implementation yes I think the the implementation yes I think the safest default implementation is just if there's no key Collision IE if you want to restore something and there's no key on the bucket list then you restore it and add to the bucket list if there is a collision just panic and fail because you don't want it to be a no up because the an archive function does delete the entry from the archive right because you can't unarchive and entry twice from the archive once you've unarchived it once it's gone forever and so you won't want to lose entries if you can't if there's a key collision and so I think the safest option that preserves all the data is for the default just if there's a collision just panic and then you either have to wait for the entry that's on the bucket list to be archived or delete the entry yourself on the bucket list and then the default default unarchive will work

00:54:00.000 --> 00:54:59.999 align:start position:0%
okay I imagine there are a lot of questions unfortunately we are at time so this discussion is going to carry on feel free to ask questions on live chat Garen if you can hang out there and ask any questions and answer any questions that'll be great and yeah thank you all this has been a very productive hour and we'll see you all next week